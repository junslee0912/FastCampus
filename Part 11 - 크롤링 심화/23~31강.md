# Part 11. 크롤링 심화
### ch 23~31. (Scrapy 프로젝트 실전)

## 23~25강. 실전 크롤링 프로젝트 - 1,2,3

### Daum News 크롤링(1)
- 사이트분석 - 페이징 처리
- Settings.py 고급 설정
- Httpcache, Headers 설명
- 데이터 수집 테스트


### class05_1.py
```python
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule
import re

# 링크 크롤링 예제(중요)
# 사이트 요구에 맞는 환경 설정 세팅(중요)
class NewsSpider(CrawlSpider): # 그냥 spider X
    name = 'test11'
    allowed_domains = ['media.daum.net']
    start_urls = ['https://media.daum.net/breakingnews/digital']

    # 링크 크롤링 규칙(정규표현식 사용 추천)
    rules = [
        # page=\d$ -> 한자리수 의미(1페이지~9페이지) 
        # page=\d+$ : 2자리수 페이지 크롤링, follow=True 속성을 넣어줘야 함
        Rule(LinkExtractor(allow=r'/breakingnews/digital\?page=\d$'), callback='parse_headline'),
    ]

    def parse_headline(self, response):
        # URL 로깅
        self.logger.info('Response URL : %s' % response.url)

        for m in response.css('ul.list_news2.list_allnews > li > div'):
            # 헤드라인
            headline = m.css('strong > a::text').extract_first().strip()
            # 본문 20글자
            contents = m.css('div > span::text').extract_first().strip()

            yield {'headline': headline, 'contents': contents}

```

### settings.py
```python
# Scrapy settings for section05_1 project
#
# For simplicity, this file contains only settings considered important or
# commonly used. You can find more settings consulting the documentation:
#
#     https://doc.scrapy.org/en/latest/topics/settings.html
#     https://doc.scrapy.org/en/latest/topics/downloader-middleware.html
#     https://doc.scrapy.org/en/latest/topics/spider-middleware.html

BOT_NAME = 'section05_1'

SPIDER_MODULES = ['section05_1.spiders']
NEWSPIDER_MODULE = 'section05_1.spiders'

# User-agent 설정, 개발자도구 참고하고 붙여넣기
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36'

# Obey robots.txt rules
ROBOTSTXT_OBEY = False

# 다운로드 간격
DOWNLOAD_DELAY = 2

# 쿠키 사용
COOKIES_ENABLED = True

# Referer 삽입
DEFAULT_REQUEST_HEADERS = {
    'Referer': 'https://media.daum.net',
}

# 재시도 횟수
RETRY_ENABLED = True
RETRY_TIMES = 2

# 한글 쓰기(출력 인코딩)
FEED_EXPORT_ENCODING = 'utf-8'

# 캐시 사용
# HTTPCACHE_ENABLED = True
# HTTPCACHE_EXPIRATION_SECS = 0
# HTTPCACHE_DIR = 'httpcache'
# HTTPCACHE_IGNORE_HTTP_CODES = []
# HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'

```

## 26~28강. 실전 크롤링 프로젝트 - 4,5,6

### Daum News 크롤링(2)
- 사이트 분석 - 상세 페이지 크롤링 추가
- Settings.py - MiddleWare 설치
- Logging 설정 및 추적
- 데이터 수집 후 파일 저장 확인

### pip install scrapy-fake-agentuser 설치

### setting.py
```python
# Scrapy settings for section05_2 project
#
# For simplicity, this file contains only settings considered important or
# commonly used. You can find more settings consulting the documentation:
#
#     https://doc.scrapy.org/en/latest/topics/settings.html
#     https://doc.scrapy.org/en/latest/topics/downloader-middleware.html
#     https://doc.scrapy.org/en/latest/topics/spider-middleware.html

BOT_NAME = 'section05_2'

SPIDER_MODULES = ['section05_2.spiders']
NEWSPIDER_MODULE = 'section05_2.spiders'

# Obey robots.txt rules
ROBOTSTXT_OBEY = False

# 다운로드 간격
DOWNLOAD_DELAY = 2

# 쿠키 사용
COOKIES_ENABLED = True

# Referer 삽입
DEFAULT_REQUEST_HEADERS = {
    'Referer': 'https://media.daum.net',
}

# User-Agent 미들웨어 사용 
DOWNLOADER_MIDDLEWARES = {
    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,
    'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware': 400,
}

# 재시도 횟수
RETRY_ENABLED = True
RETRY_TIMES = 2

# 한글 쓰기(출력 인코딩)
FEED_EXPORT_ENCODING = 'utf-8'

# 캐시 사용
# HTTPCACHE_ENABLED = True
# HTTPCACHE_EXPIRATION_SECS = 0
# HTTPCACHE_DIR = 'httpcache'
# HTTPCACHE_IGNORE_HTTP_CODES = []
# HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'

```

### items.py
```python
# -*- coding: utf-8 -*-

# Define here the models for your scraped items
#
# See documentation in:
# https://doc.scrapy.org/en/latest/topics/items.html

import scrapy

class ArticleItems(scrapy.Item):
    # 기사 제목
    headline = scrapy.Field()
    # 기사 본문
    contents = scrapy.Field()
    # 요청 리스트 페이지
    parent_link = scrapy.Field()
    # 기사 페이지
    article_link = scrapy.Field()


```

### class05_2.py
```python
import scrapy
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule

from ..items import ArticleItems


# 페이지 + 상세 페이지 크롤링 추가
# 미들웨어 설치 : pip install scrapy_fake_useragent
class NewsSpider(CrawlSpider):
    name = 'test12'
    # 허용 도메인
    allowed_domains = ['media.daum.net', 'v.media.daum.net']
    # 시작 URL(2페이지로 변경)
    start_urls = ['https://media.daum.net/breakingnews/digital']

    rules = [
        # 뉴스 메인페이지
        # 테스트 시 : page=\d$ 수정
        Rule(LinkExtractor(allow=r'/breakingnews/digital\?page=\d$'), callback='parse_parent'),
    ]

    def parse_parent(self, response):
        # 부모 상세 URL 로깅
        self.logger.info('Parent Response URL : %s' % response.url)

        # 페이지내 신문 상세 요청
        for url in response.css('ul.list_news2.list_allnews > li > div'):
            # 신문 기사 URL
            article_url = url.css('strong > a::attr(href)').extract_first().strip()
            # 요청
            yield scrapy.Request(article_url, self.parse_child, meta={'parent_url': response.url})

    def parse_child(self, response):
        # 부모, 자식 수신 정보 로깅
        self.logger.info('-------------------------------------------')
        self.logger.info('Response From Parent URL : %s' % response.meta['parent_url'])
        self.logger.info('Child Response URL : %s' % response.url)
        self.logger.info('Child Response Status : %s' % response.status)
        self.logger.info('-------------------------------------------')

        # 헤드라인
        headline = response.css('h3.tit_view::text').extract_first().strip()
        # 본문 20글자
        c_list = response.css('div.article_view p::text').extract_first().strip()
        # 리스트 -> 문자열 변경
        contents = ''.join(c_list).strip()

        yield ArticleItems(headline=headline, contents=contents,
                           parent_link=response.meta['parent_url'],
                           article_link=response.url)


```

## 29~31강. 실전 크롤링 프로젝트 - 7,8,9

### Daum News 크롤링(3)
- 크롤링 - 파이프라인 추가
- SQLite3 데이터 삽입
- Pipeline 리팩토링
- 데이터 수집 후 DB 저장 확인
- 최종 정리 - 도움이 되는 학습 추천

### 수업 전 SQLiteDatabaseBrowserPortable 설치해야합니다! - 파이썬 기초 때 설치하셨던 분들은 따로 설치 필요없습니다!

### pipelines.py
```python
import datetime
import sqlite3

from scrapy.exceptions import DropItem


class NewsSpiderPipeline(object):
    # 초기화 메소드
    def __init__(self):
        # DB 설정(자동 커밋)
        self.conn = sqlite3.connect('C:/database.db', isolation_level=None)
        # DB 연결
        self.c = self.conn.cursor()

    # 최초 1회 실행
    def open_spider(self, spider):
        spider.logger.info('NewsSpider Pipeline Stated.')
        self.c.execute(
            "CREATE TABLE IF NOT EXISTS NEWS_DATA(id INTEGER PRIMARY KEY AUTOINCREMENT, headline text, contents text, parent_link text, article_link text, crawled_time text)")

    def process_item(self, item, spider):
        if not item.get('contents') is None:
            # 삽입 시간
            crawled_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            # 크롤링 시간 필드 추가
            item['crawled_time'] = crawled_time
            # 데이터 DB 삽입
            self.c.execute('INSERT INTO NEWS_DATA(headline, contents, parent_link, article_link, crawled_time) VALUES (? ,?, ?, ?, ?);', (item.get('headline'), item.get('contents'), item.get('parent_link'), item.get('article_link'), item.get('crawled_time')))  #tuple(item[k] for k in item.keys())
            # 로그
            spider.logger.info('Item to DB Inserted')
            spider.logger.logger.info('-------------------------------------------')
            # 결과 리턴
            return item
        else:
            raise DropItem('Dropped Item. Because This Contents is Empty.')

    # 마지막 1회 실행
    def close_spider(self, spider):
        spider.logger.info('NewsSpider Pipeline Stopped.')
        # 커밋
        self.conn.commit()
        # 연결 해제
        self.conn.close()

```
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTkwMDkzNTU4OF19
-->