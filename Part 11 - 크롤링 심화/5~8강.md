# Part 11. 크롤링 심화
### ch 5~8. (Scrapy 기초)

## 5~6강. Scrapy 크롤링 실습 - 1,2

### Scrapy 블로그 크롤링
- Scrapy-Request 이해
- CSS Selector
- XPath Selector
- 블로그 Title 수집

### 실습
- scrapy startproject section02_1
- scrapy genspider class02-1 blog.scrapinghub.com
```python
import scrapy
# 개발자 도구로 확인하기
class TestSpider(scrapy.Spider):
    name = 'test2'
    allowed_domains = ['blog.scrapinghub.com']
    start_urls = ['https://blog.scrapinghub.com/']

    def parse(self, response):
        """
        :param : response 
        :return: Title Text
        """
        # 2가지(CSS Selector, XPath)
        # getall() <-> get(), extract() <-> extract_first()
        # 우리는 조건에 맞는 헤더의 텍스트들을 다 가져와야 함 -> getall() 또는 extract()
       
        # 아래 4개 다 가능
        # css
        # response.css('div.post-header h2 a::text').getall() <-> get()
        # a::text : a태그의 text만 가져와라
        # response.css('div.post-header h2 a::text').extract() <-> extract_first()
        # response.xpath('//div[@class="post-header"]/h2/a/text()').getall() <-> get()
        # response.xpath('//div[@class="post-header"]/h2/a/text()').extract() <-> extract_first()

        # 예제1(CSS Selector)
        # for text in response.css('div.post-header h2 a::text').getall():
        #     Return Type : Request, BaseItem, dictionary, None
        #     yield {'title', text}
		# cmd 파일에 scrapy crawl test2 -o result.jl -t jsonlines ---> result.jl이라는 파일이 만들어지고 타겟으로 하는 데이터 스크래핑 저장됨.
		# 출력 옵션
		# -o 파일명.확장자, -t 파일 타입(json, jsonlines, jl, csv, xml, marshal,pickle)
		
        # 예제2(XPath)
        for i, text in enumerate(response.xpath('//div[@class="post-header"]/h2/a/text()').getall()):
            # Return Type : Request, BaseItem, dictionary, None
            yield {
                'num': i,
                'text': text
            }

```

## 7~8강. Scrapy 크롤링 실습 - 3,4

### Scrapy 블로그 크롤링
- 페이지 링크 순회 
- 링크 페이지 본문 수집
- 수집 데이터 다양한 파일 형식 저장
- Get, GetAll, Extract, Extract_first

### 실습
- scrapy startproject section02_2
- scrapy genspider class02-2 blog.scrapinghub.com

```python
import scrapy

class TestSpider(scrapy.Spider):
    # 페이지 순회 크롤링 예제
    name = 'test3'
    allowed_domains = ['blog.scrapinghub.com']
    start_urls = ['https://blog.scrapinghub.com/']

    # 메인 페이지 순회
    def parse(self, response):
        """
        :param : response
        :return: Request
        """
        # response.css('div.post-item > div > a::attr("href")').getall()
        # response.css('div.post-item > div > a::attr("href")').extract()
        # response.xpath('//div[@class="post-item"]/div/a/@href').getall()
        # response.xpath('//div[@class="post-item"]/div/a/@href').extract()

        for url in response.css('div.post-item > div > a::attr("href")').getall():
            # url 보다 urljoin 사용
            yield scrapy.Request(response.urljoin(url), self.parse_title)

    # 상세 페이지 순회
    def parse_title(self, response):
        """
        상세 페이지 -> 타이틀 추출
        :param response: 
        :return Contents Text :
        """
        contents = response.css('div.post-body > span > p::text').extract()[:5]

        yield {'contents': ''.join(contents)}

```
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTEyMjYyODMzMl19
-->