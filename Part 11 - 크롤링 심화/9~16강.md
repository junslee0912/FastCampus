
# Part 11. 크롤링 심화
### ch 17~22. (Scrapy 구성 기초)

## 17~18강. Scrapy Settings - 1,2
### Scrapy Setting 설명 및 설정
- Settings.py - 다양한 설정 방법
- 각 설정 항목 상세 설명
- News 사이트 크롤링 연습
- 기타 항목(캐시, 미들웨어)

### class04_1.py
```python
# -*- coding: utf-8 -*-
import scrapy

# Scrapy 환경설정
# 중요

# 실행방법
# 1.커맨드라인 실행 -> scrapy crawl 크롤러명 -s(--set) <NAME>=<VALUE>
# -> scrapy crawl test8 -s DOWNLOAD_DELAY=3
# 2.Spider 실행 시 직접 설정
# -> 아래 소스 참조
# 3.프로젝트 설정 파일
# -> settings.py
# 4.서브 명령어 (신경 X)
# 5.글로벌 설정 : scrapy.settings.default_settings


class TestSpider(scrapy.Spider):
    name = 'test8'
    allowed_domains = ['globalvoices.org']
    start_urls = ['https://globalvoices.org/']

    # Spider 환경 설정
    # custom_settings = {
    #     'DOWNLOAD_DELAY': 3
    # }

    def parse(self, response):
        # xpath + css 혼합
        for i, v in enumerate(response.xpath('//div[@class="post-summary-content"]').css('div.post-excerpt-container > h3 > a::text').extract(), 1):
            # 인덱스 번호, 헤드라인
            yield dict(num=i, headline=v)


```

### settings.py
```python
# -*- coding: utf-8 -*-

# Scrapy settings for section04_1 project
#
# For simplicity, this file contains only settings considered important or
# commonly used. You can find more settings consulting the documentation:
#
#     https://doc.scrapy.org/en/latest/topics/settings.html
#     https://doc.scrapy.org/en/latest/topics/downloader-middleware.html
#     https://doc.scrapy.org/en/latest/topics/spider-middleware.html

BOT_NAME = 'section04_1'

SPIDER_MODULES = ['section04_1.spiders'] # 여러개의 스파이더 가능
NEWSPIDER_MODULE = 'section04_1.spiders'

# Crawl responsibly by identifying yourself (and your website) on the user-1
# UserAgent 설정
# USER_AGENT = 'section04_1 (+http://www.yourdomain.com)'

# Obey robots.txt rules
# 타겟 사이트 Robots.txt 규칙 준수 여부
ROBOTSTXT_OBEY = True

# Configure maximum concurrent requests performed by Scrapy (default: 16)
# 병렬 처리
# CONCURRENT_REQUESTS = 32

# Configure a delay for requests for the same website (default: 0)
# See https://doc.scrapy.org/en/latest/topics/settings.html#download-delay
# See also autothrottle settings and docs
# 다운로드 딜레이
# DOWNLOAD_DELAY = 3

# The download delay setting will honor only one of:
# 웹 사이트 도메인 동시 병렬 처리 개수
# CONCURRENT_REQUESTS_PER_DOMAIN = 16
# 특정 웹 사이트 IP 주소 병렬 처리 개수
# CONCURRENT_REQUESTS_PER_IP = 16

# Disable cookies (enabled by default)
# 쿠키 활성화 여부
# COOKIES_ENABLED = False
# 403 error가 뜨면, True로 바꿔놓고 해보기

# Disable Telnet Console (enabled by default)
# TELNETCONSOLE_ENABLED = False

# Override the default request headers:
# 기본 Request 헤더
# DEFAULT_REQUEST_HEADERS = {
#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
#   'Accept-Language': 'en',
# }

# Enable or disable spider middlewares
# See https://doc.scrapy.org/en/latest/topics/spider-middleware.html
# 미들웨어 사용 여부
# SPIDER_MIDDLEWARES = {
#    'section04_1.middlewares.Section041SpiderMiddleware': 543,
# }

# Enable or disable downloader middlewares
# See https://doc.scrapy.org/en/latest/topics/downloader-middleware.html
# 특정 다운로드 미들웨어 사용
# DOWNLOADER_MIDDLEWARES = {
#    'section04_1.middlewares.Section041DownloaderMiddleware': 543,
# }

# Enable or disable extensions
# See https://doc.scrapy.org/en/latest/topics/extensions.html
# 익스텐션 사용 여부
# EXTENSIONS = {
#    'scrapy.extensions.telnet.TelnetConsole': None,
# }

# Configure item pipelines
# See https://doc.scrapy.org/en/latest/topics/item-pipeline.html
# 파이프라인 설정
# ITEM_PIPELINES = {
#    'section04_1.pipelines.Section041Pipeline': 300,
# }

# Enable and configure the AutoThrottle extension (disabled by default)
# See https://doc.scrapy.org/en/latest/topics/autothrottle.html
# AUTOTHROTTLE_ENABLED = True
# The initial download delay
# AUTOTHROTTLE_START_DELAY = 5
# The maximum download delay to be set in case of high latencies
# AUTOTHROTTLE_MAX_DELAY = 60
# The average number of requests Scrapy should be sending in parallel to
# each remote server
# AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0
# Enable showing throttling stats for every response received:
# AUTOTHROTTLE_DEBUG = False

# Enable and configure HTTP caching (disabled by default)
# See https://doc.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings
# 캐시 사용 여부
# 캐시를 사용하면 동일하게 여러 번 요청 시 서버 사이드에 부하 절감 가능(변동사항 없을 경우)
# HTTPCACHE_ENABLED = True
# 캐시 유효 기간
# HTTPCACHE_EXPIRATION_SECS = 0
# 캐시 저장 경로
# HTTPCACHE_DIR = 'httpcache'
# 응답 거부 캐시
# HTTPCACHE_IGNORE_HTTP_CODES = []
# HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'

# 오류 처리
# 자동 재시도 설정
# RETRY_ENABLED = True

# 재시도 횟수 최대값
# RETRY_TIMES = 2

# 재시도 대상 HTTP 코드
# RETRY_HTTP_CODES = [500, 502, 503, 504, 408]
# 정의된 HTTP 코드을은 재시도함

# 오류 무시 HTTP 상태 코드
# HTTPERROR_ALLOWED_CODES = [404]

# 모든 상태 코드 오류 무시
# HTTPERROR_ALLOW_ALL = FALSE

```

## 19~20강. Scrapy Pipeline - 1,2

### Scrapy Pipeline 설명 및 설정
- Pipeline 설명
- Pipeline 설정 및 메소드
- Amazon Alexa 크롤링 연습
- Validation 추가 및 테스트

### Pipeline 
- 스파이더에서 수집하고, pipeline에서 필터링을 거쳐 원하는 정보를 추출하고 저장

### class04_2.py
```python
# -*- coding: utf-8 -*-
import scrapy
from ..items import SitesRankItems


class TestSpider(scrapy.Spider):
    name = 'test9'
    allowed_domains = ['alexa.com/topsites']
    start_urls = ['https://www.alexa.com/topsites/']

    def parse(self, response):
        """
        :param : response
        :return: SitesRankItems
        """
        for p in response.css('div.listings.table > div.tr.site-listing'):
            # 아이템 객체 생성
            item = SitesRankItems()
            # 개발자 도구 참고
            item['rank_num'] = p.xpath('div[@class="td"]/text()').get()
            item['site_name'] = p.xpath('div[@class="td DescriptionCell"]/p/a/text()').get()
            item['daily_time_site'] = p.xpath('div[@class="td right"]/p/text()').getall()[0]
            item['daily_page_view'] = p.xpath('div[@class="td right"]/p/text()').getall()[1]

            yield item


```

### items.py
```python
# -*- coding: utf-8 -*-

# Define here the models for your scraped items
#
# See documentation in:
# https://doc.scrapy.org/en/latest/topics/items.html

import scrapy


class SitesRankItems(scrapy.Item):
    # 제목
    rank_num = scrapy.Field()
    # 사이트 네임
    site_name = scrapy.Field()
    # 머문 시간
    daily_time_site = scrapy.Field()
    # 페이지뷰
    daily_page_view = scrapy.Field()
    # 파이프라인 통과 여부
    is_pass = scrapy.Field()

```

### settings.py
```python
# -*- coding: utf-8 -*-

# Scrapy settings for section04_2 project
#
# For simplicity, this file contains only settings considered important or
# commonly used. You can find more settings consulting the documentation:
#
#     https://doc.scrapy.org/en/latest/topics/settings.html
#     https://doc.scrapy.org/en/latest/topics/downloader-middleware.html
#     https://doc.scrapy.org/en/latest/topics/spider-middleware.html

BOT_NAME = 'section04_2'

SPIDER_MODULES = ['section04_2.spiders']
NEWSPIDER_MODULE = 'section04_2.spiders'


# Crawl responsibly by identifying yourself (and your website) on the user-agent
#USER_AGENT = 'section04_2 (+http://www.yourdomain.com)'

# Obey robots.txt rules
ROBOTSTXT_OBEY = False

# Configure maximum concurrent requests performed by Scrapy (default: 16)
#CONCURRENT_REQUESTS = 32

# Configure a delay for requests for the same website (default: 0)
# See https://doc.scrapy.org/en/latest/topics/settings.html#download-delay
# See also autothrottle settings and docs
DOWNLOAD_DELAY = 1
# The download delay setting will honor only one of:
#CONCURRENT_REQUESTS_PER_DOMAIN = 16
#CONCURRENT_REQUESTS_PER_IP = 16

# Disable cookies (enabled by default)
#COOKIES_ENABLED = False

# Disable Telnet Console (enabled by default)
#TELNETCONSOLE_ENABLED = False

# Override the default request headers:
#DEFAULT_REQUEST_HEADERS = {
#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
#   'Accept-Language': 'en',
#}

# Enable or disable spider middlewares
# See https://doc.scrapy.org/en/latest/topics/spider-middleware.html
#SPIDER_MIDDLEWARES = {
#    'section04_2.middlewares.Section042SpiderMiddleware': 543,
#}

# Enable or disable downloader middlewares
# See https://doc.scrapy.org/en/latest/topics/downloader-middleware.html
#DOWNLOADER_MIDDLEWARES = {
#    'section04_2.middlewares.Section042DownloaderMiddleware': 543,
#}

# Enable or disable extensions
# See https://doc.scrapy.org/en/latest/topics/extensions.html
#EXTENSIONS = {
#    'scrapy.extensions.telnet.TelnetConsole': None,
#}

# Configure item pipelines
# See https://doc.scrapy.org/en/latest/topics/item-pipeline.html
# 파이프 라인 활성화
# 숫자가 작을 수록 우선순위 상위
ITEM_PIPELINES = {
    'section04_2.pipelines.TestSpiderPipeline': 300,
}

# Enable and configure the AutoThrottle extension (disabled by default)
# See https://doc.scrapy.org/en/latest/topics/autothrottle.html
#AUTOTHROTTLE_ENABLED = True
# The initial download delay
#AUTOTHROTTLE_START_DELAY = 5
# The maximum download delay to be set in case of high latencies
#AUTOTHROTTLE_MAX_DELAY = 60
# The average number of requests Scrapy should be sending in parallel to
# each remote server
#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0
# Enable showing throttling stats for every response received:
#AUTOTHROTTLE_DEBUG = False

# Enable and configure HTTP caching (disabled by default)
# See https://doc.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings
#HTTPCACHE_ENABLED = True
#HTTPCACHE_EXPIRATION_SECS = 0
#HTTPCACHE_DIR = 'httpcache'
#HTTPCACHE_IGNORE_HTTP_CODES = []
#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'


```

### pipelines.py
```python
# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: https://doc.scrapy.org/en/latest/topics/item-pipeline.html

# 파이프라인 실습(1)
# 예) 잘못된 데이터 제거, DB 저장, SNS 전송, SMS 전송, 메일 전송 등

from scrapy.exceptions import DropItem


class TestSpiderPipeline(object):
    # 최초 1회 실행
    def open_spider(self, spider):
        spider.logger.info('TestSpider Pipeline Stated.')

    # 아이템 추출 시 항상 실행
    # 40위 초과는 Drop 테스트
    def process_item(self, item, spider):
        if int(item.get('rank_num')) < 41:
            item['is_pass'] = True
            return item
        else:
            raise DropItem(f'Dropped Item. Because This Site Rank is {item.get("rank_num")}')

    # 마지막 1회 실행
    def close_spider(selfs, spider):
        spider.logger.info('TestSpider Pipeline Stopped.')

```

## 21~22강. Scrapy Pipeline - 3,4

### Scrapy Pipeline 파일 저장
- Pipeline 초기화 메소드
- Item CSV 저장
- Item Excel 저장
- Pipeline 요약 설명

### pipelines.py
```python
# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: https://doc.scrapy.org/en/latest/topics/item-pipeline.html

# 파이프라인 실습(1)
# 예) 잘못된 데이터 제거, DB 저장, SNS 전송, SMS 전송, 메일 전송 등

# 엑셀 처리 임포트
import xlsxwriter
import csv
from scrapy.exceptions import DropItem


class TestSpiderPipeline(object):

    # 초기화 메소드
    def __init__(self):
        # 엑셀 처리 선언
        self.workbook = xlsxwriter.Workbook("C:/result_excel.xlsx")
        # CSV 처리 선언(a, w 옵션 변경)
        self.file_opener = open('C:/result_csv.csv', 'w')
        self.csv_writer = csv.DictWriter(self.file_opener, fieldnames=['rank_num', 'site_name', 'daily_time_site', 'daily_page_view', 'is_pass'])
        # 워크 시트
        self.worksheet = self.workbook.add_worksheet()
        # 삽입 수
        self.rowcount = 1

    # 최초 1회 실행
    def open_spider(self, spider):
        spider.logger.info('TestSpider Pipeline Stated.')

    # 아이템 추출 시 항상 실행
    # 40위 초과는 Drop 테스트
    def process_item(self, item, spider):
        if int(item.get('rank_num')) < 41:
            item['is_pass'] = True
            # 엑셀 저장
            self.worksheet.write('A%s' % self.rowcount, item.get('rank_num'))
            self.worksheet.write('B%s' % self.rowcount, item.get('site_name'))
            self.worksheet.write('C%s' % self.rowcount, item.get('daily_time_site'))
            self.worksheet.write('D%s' % self.rowcount, item.get('daily_page_view'))
            self.worksheet.write('D%s' % self.rowcount, item.get('is_pass'))
            self.rowcount += 1

            # CSV 저장
            self.csv_writer.writerow(item)

            return item
        else:
            raise DropItem(f'Dropped Item. Because This Site Rank is {item.get("rank_num")}')

    # 마지막 1회 실행
    def close_spider(self, spider):
        # 엑셀 파일 닫기
        self.workbook.close()
        # CSV 파일 닫기
        self.file_opener.close()
        spider.logger.info('TestSpider Pipeline Stopped.')

```
<!--stackedit_data:
eyJoaXN0b3J5IjpbMzUxMDYxNjMxXX0=
-->