# Part 11. 크롤링 심화
### ch 9~16. (Scrapy 구성 기초)

## 9강. Scrapy Shell
### Scrapy Shell 활용하기
- Shell 사용 장점
- 명령어 사용 실습
- 대상 데이터 수집 실습(css, xpath)
- Fetch, View, response, request

### Shell 모드 사용법
- cmd / 터미널
		1. scrapy shell / scrapy shell '주소'
		2. fetch('주소') : shell 모드에서 request하는 대상을 바꿈
		3. view(response) : response 데이터에 뭐가 들어있는지 알려줌, 만약 네이버 주소를 fetch 했다면, 네이버가 컴퓨터에 저장되어 열림.
		4. response.body : 현재 body 정보
		5. response.css('div.post-item>div>a::attr("href")').getall()
		6. response.xpath('//div[@class="post-item"]/div/a/@href').getall()
		7. dir(response)를 통해 메소드들을 활용가능
		8. scrapy shell https://daum.net --set="ROBOTSTXT_OBEY=False" -> settings를 변경하면서 실행할 수 있음, 네이버 다음의 경우 False로 해야 나옴

## 10~11강. Scrapy Spider - 1,2
### Scrapy Spider 활용하기
- Spider Attribute
- Multi Domain
- Logger
- Response 분기

### 실습
```python
import scrapy
import logging

# 커맨시 실행시 --logfile 파일명, --nolog
logger = logging.getLogger('mylogger1')


# 스파이더 종류 : CrawlSpider, XMLFeedSpider, CSVFeedSpider, SitemapSpider
class TestSpider(scrapy.Spider):
    # 스파이더 네임 (중복 허용X, 스파이더 실행 시 사용)
    name = 'test4'
    # 크롤링 허용 도메인(리스트 가능)
    allowed_domains = ['blog.scrapinghub.com', 'daum.net', 'naver.com']
    # 크롤링 대상 URLS(멀티 사용 가능)
    # 싱글 도메인
    # start_urls = ['http://blog.scrapinghub.com/']

    # 실행 방법1
    # 멀티 도메인
    start_urls = ['http://blog.scrapinghub.com/', 'https://naver.com', 'https://daum.net']

    # 사용자 시스템 설정(settings.py)
    custom_settings = {
        'DOWNLOAD_DELAY': 1
    }

    # 실행 방법2(미리 정의된 함수 사용)
    # Request 각각 지정
    # def start_requests(self):
    #     yield scrapy.Request('http://blog.scrapinghub.com/', self.parse)
    #     yield scrapy.Request('https://naver.com', self.parse)
    #     yield scrapy.Request('https://daum.net', self.parse)

    def parse(self, response):
        # 방법1(파이썬 패키지)
        logger.info('Response URL1 : %s' % response.url)
        logger.info('Response Status Code1 : %s' % response.status)

        # 방법2(Spider logger 사용)
        self.logger.info('Response URL2 : %s' % response.url)
        self.logger.info('Response Status Code2 : %s' % response.status)

        if response.url.find('scrapinghub'):
            yield {
                'sitename': response.url,
                'contents': response.text[:100]
            }
        elif response.url.find('naver'):
            yield {
                'sitename': response.url,
                'contents': response.text[:100]
            }
        else:
            yield {
                'sitename': response.url,
                'contents': response.text[:100]
            }


```

## 12~13강. Scrapy Selectors - 1,2

### Scrapy Selector 사용
- xpath Selector
- CSS Selector
- W3school 크롤링 연습
- Selectors functions 비교 설명

### 실습
```python
import scrapy

# xpath 선택자 도움 사이트
# https://docs.scrapy.org/en/latest/topics/selectors.html#working-with-xpaths
# http://www.nextree.co.kr/p6278/

# css 선택자 도움 사이트
# https://docs.scrapy.org/en/latest/topics/selectors.html#extensions-to-css-selectors

# 정규 표현식 중요
# https://www.w3schools.com/python/python_regex.asp

# 타겟 데이터는 크롬 개발자 도구 사용

# 선택자 연습 팁 : scrapy shell에서 테스트(효율성)
# scrapy shell 도메인

class TestSpider(scrapy.Spider):
    # 스파이더 이름(실행)
    name = 'test5'
    # 허용 도메인
    allowed_domains = ['w3schools.com/']
    # 시작 URL
    start_urls = ['https://www.w3schools.com/']

    # CSS 선택자
    # A B : 자손
    # A > B : 자식
    # ::text -> 노드 텍스트만 추출
    # ::attr(name) -> 노드 속성 값 추출
    # get(), getall() 사용 숙지
    # get(default='') 사용 가능

    # 예)
    # response.css('title::text').get() : 타이틀 태그의 텍스트만 추출
    # response.css('div > a::attr(href)').getall() : div 태그의 자식 a 태그 href 속성 값 전부 추출

    # Xpath 선택자
    # nodename : 이름이 nodename 선택
    # text() : -> 노드 텍스트만 추출
    # / : 루트부터 시작
    # // : 현재 node 부터 문서상의 모든 노드 조회
    # . : 현재 node
    # .. : 현재 node의 부모 노드
    # @ : 속성 선택자
    # excract(), extract_first() 사용 숙지

    # 예)
    # response.xpath('//div') : 루트 노드부터 모든 div 태그 선택
    # response.xpath('//div[@id="id1"]/a/text()').get() : div 태그 중 id가 'id1' 인 자식 a 태그의 텍스트 추출

    # 중요
    # get() == extract_first()
    # getall() == extract()

    # 혼합 사용 가능
    # response.css('img').xpath('@src').getall()

    # nav 메뉴 이름 크롤링 실습
    # 쉘 실행 -> 선택자 확인 -> 코딩 -> 데이터 저장
    def parse(self, response):
        # 둘다 가능
        # response.css('nav#mySidenav > div a::text').getall()
        # response.css('nav#mySidenav').xpath('./div//a/text()').extract()

        for n, text in enumerate(response.css('nav#mySidenav > div a::text').getall(), 1):
            # Return Type : Request, BaseItem, dictionary, None
            yield {'num': n, 'learn Title': text}

```
## 14~15강. Scrapy Items - 1,2

### Scrapy Items 설명 및 크롤링
- Scrapy Items 사용 이유
- IT News 사이트 크롤링 연습
- 메인 페이지 -> 상세페이지 크롤링
- Item 선언 및 수집 데이터 매핑

### items.py
```python
# Scrapy Item
# 장점 
# 1. 수집 데이터를 일관성있게 관리 가능
# 2. 데이터를 사전형(Dict)로 관리, 오타 방지
# 3. 추후 가공 및 DB 저장 용이

import scrapy

# Scrapy Item 연습
class ItArticle(scrapy.Item):
    # 제목
    title = scrapy.Field()
    # 이미지 URL
    img_url = scrapy.Field()
    # 본문 내용
    contents = scrapy.Field()

```
### class03_4.py
```python
import scrapy 
from ..items import ItArticle

# Scrapy Item 실습
# https://www.itnews.com/ 크롤링 연습

class TestSpider(scrapy.Spider):
    name = 'test6'
    allowed_domains = ['itnews.com']
    start_urls = ['https://itnews.com/']

    # 메인 페이지 순회
    def parse(self, response):
        """
        :param : response
        :return: Request
        """

        for url in response.css('div.news-item > div.hed > div.title > a::attr(href)').extract():
            yield scrapy.Request(response.urljoin(url), self.parse_article)

    # 상세 페이지 순회
    def parse_article(self, response):
        """
        상세 페이지 -> 타이틀, URL, 컨텐츠 추출
        :param response:
        :return Items :
        """

        # 아이템 객체 생성
        item = ItArticle()
        item['title'] = response.xpath('//h1[@itemprop="headline"]/text()').get()
        item['img_url'] = response.xpath('//img[@itemprop="contentUrl"]/@src').get()
        item['contents'] = ''.join(response.xpath('//div[@itemprop="articleBody"]/p/text()').getall())

        # dict 자료형의 모든 메소드 사용 가능
        # print(dict(item))
        # print(dir(dict(item)))
        yield item
```

## 16강. Scrapy Exports

### Section03-4 복사 붙여넣고 03-5로 바꾸기(폴더이름, scrapy.cfg, settings.py도 바꿔줄것!)

### Scrapy 크롤링 결과 파일 저장
- 저장 가능 파일 형식
- Command 명령어 파일 저장
- Settings.py 설정 파일 저장
- Feed_Export 관련 옵션 설명

### class03_5.py
```python
import scrapy

from ..items import ItArticle


# Scrapy Feed Export 실습
# https://www.itnews.com/ 크롤링 연습

# 출력 형식
# JSON, JSON lines
# CSV
# XML
# Pickle, Marshal

# 저장 위치
# Local filesystem - (My PC)
# FTP - (Server)
# S3 - (Amazon)
# Standard Console - 기본 콘솔

# 방법 2가지
# 1. 커맨드 이용 (--output, -o + --output-format, -t)
#    로컬 경로시 예) file:///C:/test.csv
#    옵션 설정 예) --set=FEED_EXPORT_INDEX=2 or -s set=FEED_EXPORT_INDEX=2
# 2. Settings.py 설정

class TestSpider(scrapy.Spider):
    name = 'test7'
    allowed_domains = ['itnews.com']
    start_urls = ['https://itnews.com/']

    # 메인 페이지 순회
    def parse(self, response):
        """
        :param : response
        :return: Request
        """

        for url in response.css('div.news-item > div.hed > div.title > a::attr(href)').extract():
            yield scrapy.Request(response.urljoin(url), self.parse_article)

    # 상세 페이지 순회
    def parse_article(self, response):
        """
        상세 페이지 -> 타이틀, URL, 컨텐츠 추출
        :param response:
        :return Items :
        """

        # 아이템 객체 생성
        item = ItArticle()
        item['title'] = response.xpath('//h1[@itemprop="headline"]/text()').get()
        item['img_url'] = response.xpath('//img[@itemprop="contentUrl"]/@src').get()
        item['contents'] = ''.join(response.xpath('//div[@itemprop="articleBody"]/p/text()').getall())

        yield item


```
### settings.py
```python
# -*- coding: utf-8 -*-

# Scrapy settings for section03_4 project
#
# For simplicity, this file contains only settings considered important or
# commonly used. You can find more settings consulting the documentation:
#
#     https://doc.scrapy.org/en/latest/topics/settings.html
#     https://doc.scrapy.org/en/latest/topics/downloader-middleware.html
#     https://doc.scrapy.org/en/latest/topics/spider-middleware.html

BOT_NAME = 'section03_4'


SPIDER_MODULES = ['section03_5.spiders']
NEWSPIDER_MODULE = 'section03_5.spiders'

# Crawl responsibly by identifying yourself (and your website) on the user-agent
# USER_AGENT = 'section03_5 (+http://www.yourdomain.com)'

# Obey robots.txt rules
ROBOTSTXT_OBEY = True

# Configure maximum concurrent requests performed by Scrapy (default: 16)
# CONCURRENT_REQUESTS = 32

# Configure a delay for requests for the same website (default: 0)
# See https://doc.scrapy.org/en/latest/topics/settings.html#download-delay
# See also autothrottle settings and docs
DOWNLOAD_DELAY = 1
# The download delay setting will honor only one of:
# CONCURRENT_REQUESTS_PER_DOMAIN = 16
# CONCURRENT_REQUESTS_PER_IP = 16

# Disable cookies (enabled by default)
# COOKIES_ENABLED = False

# Disable Telnet Console (enabled by default)
# TELNETCONSOLE_ENABLED = False

# Override the default request headers:
# DEFAULT_REQUEST_HEADERS = {
#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
#   'Accept-Language': 'en',
# }

# Enable or disable spider middlewares
# See https://doc.scrapy.org/en/latest/topics/spider-middleware.html
# SPIDER_MIDDLEWARES = {
#    'section03_5.middlewares.Section034SpiderMiddleware': 543,
# }

# Enable or disable downloader middlewares
# See https://doc.scrapy.org/en/latest/topics/downloader-middleware.html
# DOWNLOADER_MIDDLEWARES = {
#    'section03_5.middlewares.Section034DownloaderMiddleware': 543,
# }

# Enable or disable extensions
# See https://doc.scrapy.org/en/latest/topics/extensions.html
# EXTENSIONS = {
#    'scrapy.extensions.telnet.TelnetConsole': None,
# }

# Configure item pipelines
# See https://doc.scrapy.org/en/latest/topics/item-pipeline.html
# ITEM_PIPELINES = {
#    'section03_5.pipelines.Section034Pipeline': 300,
# }

# Enable and configure the AutoThrottle extension (disabled by default)
# See https://doc.scrapy.org/en/latest/topics/autothrottle.html
# AUTOTHROTTLE_ENABLED = True
# The initial download delay
# AUTOTHROTTLE_START_DELAY = 5
# The maximum download delay to be set in case of high latencies
# AUTOTHROTTLE_MAX_DELAY = 60
# The average number of requests Scrapy should be sending in parallel to
# each remote server
# AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0
# Enable showing throttling stats for every response received:
# AUTOTHROTTLE_DEBUG = False

# Enable and configure HTTP caching (disabled by default)
# See https://doc.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings
# HTTPCACHE_ENABLED = True
# HTTPCACHE_EXPIRATION_SECS = 0
# HTTPCACHE_DIR = 'httpcache'
# HTTPCACHE_IGNORE_HTTP_CODES = []
# HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'


# 출력(Export) 설정
# 파일 이름 및 경로(file or ftp, s3)
# FEED_URI = 'result.json'

# 파일 형식(json, jsonlines, csv, xml, pickle, marshal)
# FEED_FORMAT = 'json'

# 출력 인코딩
# FEED_EXPORT_ENCODING = 'utf-8'

# 기본 들여쓰기
# FEED_EXPORT_INDENT = 0

# 저장소, 저장형식 관련 세팅은 레퍼런스 참조
# https://docs.scrapy.org/en/latest/topics/feed-exports.html#feed-storages

```

<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE1NjYyMjcwMzRdfQ==
-->