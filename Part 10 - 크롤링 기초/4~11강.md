# Part 10. 크롤링 기초
### ch 4~11. (기본 스크랩핑)

## 4~5강. urllib 사용법 및 기초 스크랩핑 - 1, 2

- 환경설정
	1. 관리자 권한으로 cmd 실행
	2. python -m venv python_crawl -> cd python_crawl -> cd Scripts (mac은 bin)-> activate -> code
	3. view - palette에서 python interpreter에 방금 만든 파일 선택
	4. task: configure task -> create -> others

### 실습
```python

# Section02-1
# 파이썬 크롤링 기초
# urllib 사용법 및 기본 스크랩핑

import urllib.request as req

# 파일 URL
# 이미지 다운 
img_url =  "http://post.phinf.naver.net/20160621_169/1466482468068lmSHj_JPEG/If7GeIbOPZuYwI-GI3xU7ENRrlfI.jpg" #-> 이미지 주소 복사
html_url = "http://google.com"

# 다운받을 경로
save_path1 = "c:/test1.jpg" # MAC의 경우 users/ 
save_path2 = "c:/index.html"

# 예외 처리
try:
    file1, header1 = req.urlretrieve(img_url, save_path1)
    file2, header2 = req.urlretrieve(html_url, save_path2)
except Exception as e:
    print("Download failed.")
    print(e)
else:
    # Header 정보 출력
    # print(header1)
    # print(header2)
    # http 통신은 한번 요청과 수신을 하면 연결이 끊김
    
    # 다운로드 파일 정보
    print("Filename1 {}".format(file1))
    print("Filename2 {}".format(file2))
    print()
    
    # 성공
    print("Download Succeed.")

```

## 6강. urlopen 함수 기초 사용법

- url.request 예외 처리
	- 기존 소스 코드 변경
	- 예외 처리 추가
	- 기타 리팩토링

### 실습
```python
# Section02-2
# 파이썬 크롤링 기초
# urlopen 함수 기초 사용법

import urllib.request as req
from urllib.error import URLError, HTTPError # 에러처리를 위해

# 다운로드 경로 및 파일명
path_list = ["c:/test1.jpg", "c:/index.html"]

# 다운로드 리소스 URL
target_url = ["http://post.phinf.naver.net/20160621_169/1466482468068lmSHj_JPEG/If7GeIbOPZuYwI-GI3xU7ENRrlfI.jpg",
              "http://google.com"]

for i, url in enumerate(target_url):
    # 예외 처리
    try:
        # 웹 수신 정보 읽기
        response = req.urlopen(url) # retrieve의 경우 웹에 있는 데이터를 바로 다운,  urlopen은 다운x, 그러나 다른인자에 넣어줄 수가 있음
        
        # 수신 내용
        contents = response.read()

        print('---------------------------------------------------')

        # 상태 정보 중간 출력
        print('Header Info-{} : {}'.format(i, response.info()))
        print('HTTP Status Code : {}'.format(response.getcode()))
        print()
        print('---------------------------------------------------')

        # 파일 쓰기
        with open(path_list[i], 'wb') as c:
            c.write(contents)

        # HTTP 에러 발생 시, 서버가 죽었거나 403 에러 등
    except HTTPError as e:
        print("Download failed.")
        print('HTTPError Code : ', e.code)

        # URL 에러 발생 시, 잘못된 url 사용 시
    except URLError as e:
        print("Download failed.")
        print('URL Error Reason : ', e.reason)

        # 성공
    else:
        print()
        print("Download Succeed.")

```

## 7~9강. lxml 사용 기초 스크랩핑 A - 1,2,3

### 설치
1. activate 된 상태에서 pip install lxml
2. pip가 낮아서 설치가 안된다면 pip install --upgrade pip
3. pip install requests, pip install cssselect

### 네이버 뉴스 스텐드 스크래핑(1)
- 네이버  메인 뉴스 정보 스크래핑
- 신문사 정보 리스트 출력
- CSS 선택자 활용

### 실습
```python
# Section02-3
# 파이썬 크롤링 기초
# lxml 사용 기초 스크랩핑(1)

import requests
import lxml.html


def main():
    """
    네이버 메인 뉴스 스탠드 스크랩핑 메인 함수
    """
    # 세션 사용 권장
    # session = requests.Session()
    # session.get('https://www.naver.com/') # urlopen써도 됨
    # Get, Post

    # 스크랩핑 대상 URL
    response = requests.get('https://www.naver.com/')
    # 신문사 링크 리스트 획득
    urls = scrape_news_list_page(response)
    # 결과 출력
    for url in urls:
        print(url)


def scrape_news_list_page(response):
    # URL 리스트 선언
    urls = []
    # 태그 정보 문자열 저장
    root = lxml.html.fromstring(response.content)

    # 문서내 경로 절대 경로 변환
    # root.make_links_absolute(response.url)

    for a in root.cssselect('.api_list .api_item a.api_link'):
	    # 구글 개발자 도구에서 우리가 크롤링하려는 부분을 확인
	    # .api_list 하위에(자손) .api_item에 a태그에 api_link를 다 가져와라
        # 링크 
        url = a.get('href')
        # 리스트 삽입
        urls.append(url)
    return urls
# cmd에서 pip install cssselct 설치

# 스크랩핑 시작
if __name__ == '__main__':
    main()


```

## 10~11강. lxml 사용 기초 스크랩핑 B - 1, 2

### 네이버 뉴스 스탠드 스크래핑(2)
- 네이버 메인 뉴스 정보 스크래핑
- 신문사 정보 딕셔너리 출력
- Session 사용
- Xpath 사용

###  코드 실습
```python
# Section02-4
# 파이썬 크롤링 기초
# lxml 사용 기초 스크랩핑(2)
# session을 사용하면 일정한 시간 동안 연결의 흐름을 유지할 수 있음(로그인)

import requests
from lxml.html import fromstring, tostring
# fromstring은 웹에서 수신된 데이터를 스트링으로 변환


def main():
    """
    네이버 메인 뉴스 스탠드 스크랩핑 메인 함수
    """
    # 세션 사용
    session = requests.Session()

    # 스크랩핑 대상 URL
    response = session.get('http://www.naver.com/')
    # 신문사 정보 딕셔너리 획득
    urls = scrape_news_list_page(response)

    # 딕셔너리 확인
    # print(urls)

    # 결과 출력
    for name, url in urls.items():
        print(name, url)


def scrape_news_list_page(response):
    # URL 딕셔너리 선언
    urls = {}
    # 태그 정보 문자열 저장
    root = fromstring(response.content)

    # 문서내 경로 절대 경로 변환
    root.make_links_absolute(response.url)

    for a in root.xpath('//ul[@class="api_list"]/li[@class="api_item"]/a[@class="api_link"]'):
        # a 구조 확인
        # print(dir(a))

        # a 문자열 출력
        # print(tostring(a, pretty_print=True))

        # 신문사, 링크 추출 함수
        name, url = extract_contents(a)

        # 딕셔너리 삽입
        urls[name] = url
    return urls


def extract_contents(dom):
    # 링크 주소
    link = dom.get('href')
    # dom 구조 확인
    # print(tostring(dom, pretty_print=True))

    # 신문사 명
    name = dom.xpath('./img')[0].get('alt')  # xpath('./img')
    return name, link


#  스크랩핑 시작
if __name__ == '__main__':
    main()

``` 
<!--stackedit_data:
eyJoaXN0b3J5IjpbNzI3OTYzNzExXX0=
-->