# Part 10. 크롤링 기초
### ch 12~17. (기본 스크랩핑 실습)

## 12~13강. Get 방식 데이터 통신 - 1, 2

### 사이트 요청 정보 확인
- encar(엔카) 사이트 정보 수신
- Get 파라미터 요청
- 수신 데이터 디코딩(Decoding)
- 요청 URL 정보 분석

### 실습
```python
# Section03-1
# 기본 스크랩핑 실습
# Get 방식 데이터 통신(1)

import urllib.request
from urllib.parse import urlparse

# 기본 요청1(encar)
url = "http://www.encar.com/"

mem = urllib.request.urlopen(url)

# 여러 정보
print('type : {}'.format(type(mem)))
print("geturl : {}".format(mem.geturl()))
print("status : {}".format(mem.status))
print("headers : {}".format(mem.getheaders()))
print("getcode : {}".format(mem.getcode()))
print("read : {}".format(mem.read(1).decode('utf-8'))) # 바이트 수
print('parse : {}'.format(urlparse('http://www.encar.co.kr?test=test').query))

# 기본 요청2(ipify)
API = "https://api.ipify.org"

# Get 방식 Parameter
values = {
    'format': 'json'
}

print('before param : {}'.format(values))
params = urllib.parse.urlencode(values)
print('after param : {}'.format(params))

# 요청 URL 생성
url = API + "?" + params
print("요청 url= {}".format(url))

# 수신 데이터 읽기
data = urllib.request.urlopen(url).read()

# 수신 데이터 디코딩
text = data.decode("utf-8")
print('response : {}'.format(text))


```

## 14강. Get 방식 데이터 통신 - 3

### 행정안전부 사이트 RSS 데이터 수신
- RSS란?
- 반복문을 활용한 연속 요청
- 요청 URL 정보 분석
- 수신 XML 데이터 확인

### 실습
```python
# Section03-2
# 기본 스크랩핑 실습
# Get 방식 데이터 통신(2) - RSS

import urllib.request
import urllib.parse

# 행정 안전부 : https://www.mois.go.kr
# 행정 안전부 RSS API URL
API = "http://www.mois.go.kr/gpms/view/jsp/rss/rss.jsp" # 이 뒷부분은 동적으로 코딩

params = []

for num in [1001, 1012, 1013, 1014]:
    params.append(dict(ctxCd=num))

# 중간 확인
# print(params)

# 연속해서 4회 요청
for c in params:
    # 파라미터 출력
    # print(c)
    # URL 인코딩
    param = urllib.parse.urlencode(c)
    # ctxCd=1001 이런식으로 나옴
    
    # URL 완성
    url = API + "?" + param
    # URL 출력
    print("url=", url)
    # 요청
    res_data = urllib.request.urlopen(url).read()
    # 수신 후 디코딩
    contents = res_data.decode("utf-8")
    # 출력
    print(contents)

```

## 15~17강. 다음사이트 주식 정보 가져오기 - 1,2,3

###  강의 참고!
1. 어떤 url로 요청을 했을 때 원하는 데이터가 없으면, 다른 url을 개발자도구의 network를 통해 살펴봐야 함
2. header 정보에서 referer와 user-agent를 통해 헤더정보를 직접 만들어서 넘겨야 함.

### 개발자 도구 송수신 분석 및 실습
- 다음 주식 정보 분석
- Fake-UserAgent 사용
- Header 정보 삽입
- 수신 데이터 가공 및 추출

### 실습 전 설치
- pip install fake-user agent

### 실습
```python
# Section03-3
# 기본 스크랩핑 실습
# 다음 주식 정보 가져오기

import json
import urllib.request as req
from fake_useragent import UserAgent


# Fake Header 정보(가상으로 User-Agent 생성)
ua = UserAgent()
# print(ua.ie)
# print(ua.msie)
# print(ua.chrome)
# print(ua.safari)
# print(ua.random)

# 헤더 선언
headers = {
    'User-Agent': ua.ie,
    'referer': 'https://finance.daum.net/'
}

# 다음 주식 요청 URL
url = "https://finance.daum.net/api/search/ranks?limit=10"

# 요청
res = req.urlopen(req.Request(url, headers=headers)).read().decode('utf-8')

# 응답 데이터 확인(Json Data)
# print('res', res)

# 응답 데이터 str -> json 변환 및 data 값 저장
rank_json = json.loads(res)['data']

# 중간 확인
print('중간 확인 : ', rank_json, '\n')

for elm in rank_json:
    # print(type(elm)) #Type 확인
    print('순위 : {}, 금액 : {}, 회사명 : {}'.format(elm['rank'], elm['tradePrice'], elm['name']), )

```

## 18~19강. request 사용 스크랩핑 A - 1

### Requests 요청 정보 payload
- 세션 활성화 및 비활성화
- 쿠키 정보 전송
- User-Agent 정보 전송
- 수신 상태 코드 확인

```python
# Section04-1
# Requests
# requests 사용 스크랩핑(1) - Session

import requests

# 세션 활성화
s = requests.Session()
# r = s.get('https://www.naver.com')

# 수신 데이터
# print('1', r.text)

# 수신 상태 코드
# print('Status Code : {}'.format(r.status_code))

# 확인
# print('OK? : {}'.format(r.ok)) # ok가 true가 나오면 정확하게 수신했다는 뜻

# 세션 비활성화
# s.close()

s = requests.Session()

# 쿠키 Return
# r = s.get('http://httpbin.org/cookies', cookies={'name': 'niceman'})
# print(r.text)

# 쿠키 Set
# r = s.get('http://httpbin.org/cookies/set', cookies={'name': 'niceman'})
# print(r.text)

# User-Agent
# url = 'http://httpbin.org/get'
# headers = {'user-agent': 'niceman_app_v1.0.0'}

# Header 정보 전송
# r = s.get(url, headers=headers)
# print(r.text)

# 세션 비활성화
s.close()

# With 문 사용
with requests.Session() as s:
    pass
    # r = s.get('https://www.naver.com')
    # print(r.text)


```
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTEyODMxMTQyODldfQ==
-->